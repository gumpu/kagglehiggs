# Questions and Ideas

## Open Questions

1. How does xgboost use the weights in its algorithm?
1. How are the missing values distributed over the various factors?
1. Why is the cutoff point for xgboost 15% signal, while the training
   data has 35% signal?
1. How does boosting work?


## Closed Questions

1. Are the testset and training set drawn from the same dataset?
   Yes they are. The density plots for the various factors look
   the same, and this is unlikely to happen by chance.


## Ideas


vi: spell spl=en ft=markdown
